{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XIezCUHnUpMX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gG8x5oNoUsan"
   },
   "outputs": [],
   "source": [
    "class ContractiveAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContractiveAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder_fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.encoder_fc2 = nn.Linear(64, 32)\n",
    "        self.bottleneck = nn.Linear(32, 16)  # bottleneck layer\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(16, 32)\n",
    "        self.decoder_fc2 = nn.Linear(32, 64)\n",
    "        self.output_fc = nn.Linear(64, 28 * 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.encoder_fc1(x))\n",
    "        x = F.relu(self.encoder_fc2(x))\n",
    "        h = F.relu(self.bottleneck(x))  # Latent representation (bottleneck)\n",
    "\n",
    "        x = F.relu(self.decoder_fc1(h))\n",
    "        x = F.relu(self.decoder_fc2(x))\n",
    "        x = self.output_fc(x)\n",
    "        x = x.view(x.size(0), 1, 28, 28)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cO2YViXUUweP"
   },
   "outputs": [],
   "source": [
    "def contractive_loss(x, x_reconstructed, h, model, Lambda=100):\n",
    "    # Reconstruction loss\n",
    "    mse_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    \n",
    "    # Contractive loss\n",
    "    dh = h * (1 - h)\n",
    "    W = model.bottleneck.weight\n",
    "    contractive = Lambda * torch.sum(dh ** 2 * torch.sum(W ** 2, dim=1))\n",
    "\n",
    "    total_loss = mse_loss + contractive\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CAE(trainloader, testloader, trainset, testset, num_epochs, batch_size, model, optimizer, device):\n",
    "    loss_array = []\n",
    "    Lambda = 100  # Contractive regularization weight\n",
    "    dataiter = iter(testloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x_batch, label) in enumerate(trainloader):\n",
    "            x_batch = x_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            x_reconstructed, h = model(x_batch)\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            loss = contractive_loss(x_batch, x_reconstructed, h, model, Lambda)\n",
    "            loss_array.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqJl00XSU0lr",
    "outputId": "3a4d15f6-52e6-4fdc-ba67-73b2fc8b6aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:08<00:00, 2.94MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 185kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:04<00:00, 958kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Epoch [1/15], Loss: 637.8931274414062\n",
      "Epoch [2/15], Loss: 682.865478515625\n",
      "Epoch [3/15], Loss: 745.5244750976562\n",
      "Epoch [4/15], Loss: 550.4984741210938\n",
      "Epoch [5/15], Loss: 474.07269287109375\n",
      "Epoch [6/15], Loss: 584.8203125\n",
      "Epoch [7/15], Loss: 559.2866821289062\n",
      "Epoch [8/15], Loss: 513.103759765625\n",
      "Epoch [9/15], Loss: 681.3972778320312\n",
      "Epoch [10/15], Loss: 545.6184692382812\n",
      "Epoch [11/15], Loss: 476.4811096191406\n",
      "Epoch [12/15], Loss: 569.795654296875\n",
      "Epoch [13/15], Loss: 473.32342529296875\n",
      "Epoch [14/15], Loss: 553.1483764648438\n",
      "Epoch [15/15], Loss: 509.8800354003906\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ContractiveAutoencoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 32\n",
    "num_epochs = 15\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "train_CAE(trainloader, testloader, trainset, testset, num_epochs, batch_size, model, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tdiVGtaTI9KT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1s3d2uGkJTG-"
   },
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(loader, device, model, loss_function, optimizer, epochs, outputs, losses):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        for image in loader:\n",
    "            # Flattening the image\n",
    "            image = image.reshape(-1, 28 * 28).to(device)\n",
    "\n",
    "            # Sending the image to the device\n",
    "            image = image.to(\"mps\")\n",
    "\n",
    "            # Output of Autoencoder\n",
    "            reconstructed = model(image)\n",
    "\n",
    "            # Calculating the loss function\n",
    "            loss = loss_function(reconstructed, image)\n",
    "\n",
    "            # The gradients are set to zero,\n",
    "            # the gradient is computed and stored.\n",
    "            # .step() performs parameter update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Storing the losses in a list for plotting\n",
    "            losses.append(loss)\n",
    "        outputs.append((epochs, image, reconstructed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "l8Tdh6VDJbEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain_AE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mtrain_AE\u001b[1;34m(loader, device, model, loss_function, optimizer, epochs, outputs, losses)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Flattening the image\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Sending the image to the device\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "tensor_transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root = \"./data\", train = True, download = True, transform = tensor_transform)\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = 30, shuffle = True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE().to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 40\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "train_AE(loader, device, model, loss_function, optimizer, epochs, outputs, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3\" pygame matplotlib\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "env.obs_type = \"grayscale\"\n",
    "observation, info = env.reset()\n",
    "Image.fromarray(observation)\n",
    "device = torch.device(\"cpu\")\n",
    "height = observation.shape[0]\n",
    "width = observation.shape[1]\n",
    "channels = observation.shape[2]\n",
    "process = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "observation = process(observation)\n",
    "observation = observation.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 256, 11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, height=height, width=width):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent), latent.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "VAE_model = VAE().to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters())\n",
    "\n",
    "losses = []\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    observation = process(observation).unsqueeze(0).to(device)\n",
    "    reconstruction, latent = VAE_model(observation)\n",
    "    loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
