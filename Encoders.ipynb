{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "XIezCUHnUpMX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "gG8x5oNoUsan"
   },
   "outputs": [],
   "source": [
    "class ContractiveAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContractiveAutoencoder, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder_fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.encoder_fc2 = nn.Linear(64, 32)\n",
    "        self.bottleneck = nn.Linear(32, 16)  # bottleneck layer\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(16, 32)\n",
    "        self.decoder_fc2 = nn.Linear(32, 64)\n",
    "        self.output_fc = nn.Linear(64, 28 * 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.encoder_fc1(x))\n",
    "        x = F.relu(self.encoder_fc2(x))\n",
    "        h = F.relu(self.bottleneck(x))  # Latent representation (bottleneck)\n",
    "\n",
    "        x = F.relu(self.decoder_fc1(h))\n",
    "        x = F.relu(self.decoder_fc2(x))\n",
    "        x = self.output_fc(x)\n",
    "        x = x.view(x.size(0), 1, 28, 28)\n",
    "\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "cO2YViXUUweP"
   },
   "outputs": [],
   "source": [
    "def contractive_loss(x, x_reconstructed, h, model, Lambda=100):\n",
    "    # Reconstruction loss\n",
    "    mse_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    \n",
    "    # Contractive loss\n",
    "    dh = h * (1 - h)\n",
    "    W = model.bottleneck.weight\n",
    "    contractive = Lambda * torch.sum(dh ** 2 * torch.sum(W ** 2, dim=1))\n",
    "\n",
    "    total_loss = mse_loss + contractive\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CAE(testloader, model, device, Lambda):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, label in testloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            x_reconstructed, h = model(x_batch)\n",
    "            loss = contractive_loss(x_batch, x_reconstructed, h, model, Lambda)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_test_loss = test_loss / len(testloader.dataset)\n",
    "    model.train()  # Switch back to training mode\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CAE(trainloader, testloader, trainset, testset, num_epochs, batch_size, model, optimizer, device, log_dir=\"./tensorboard_logs\"):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    Lambda = 100  # Contractive regularization weight\n",
    "    dataiter = iter(testloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for i, (x_batch, label) in enumerate(trainloader):\n",
    "            x_batch = x_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            x_reconstructed, h = model(x_batch)\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            loss = contractive_loss(x_batch, x_reconstructed, h, model, Lambda)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(trainloader.dataset)\n",
    "        writer.add_scalar(\"CAE Loss/Train\", avg_train_loss, epoch)\n",
    "\n",
    "        # Log validation loss\n",
    "        test_loss = evaluate_CAE(testloader, model, device, Lambda)\n",
    "        writer.add_scalar(\"CAE Loss/Test\", test_loss, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqJl00XSU0lr",
    "outputId": "3a4d15f6-52e6-4fdc-ba67-73b2fc8b6aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 643.9605102539062\n",
      "Epoch [2/5], Loss: 654.4360961914062\n",
      "Epoch [3/5], Loss: 600.0396118164062\n",
      "Epoch [4/5], Loss: 547.0094604492188\n",
      "Epoch [5/5], Loss: 653.9487915039062\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ContractiveAutoencoder().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "train_CAE(trainloader, testloader, trainset, testset, num_epochs, batch_size, model, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "1s3d2uGkJTG-"
   },
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28 * 28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 28 * 28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(loader, device, model, loss_function, optimizer, epochs, log_dir=\"./tensorboard_logs\"):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0  # Accumulate loss for averaging\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(loader):  # Unpack the data and ignore labels\n",
    "            # Flattening the image\n",
    "            image = data.reshape(-1, 28 * 28).to(device)\n",
    "\n",
    "            # Output of Autoencoder\n",
    "            reconstructed = model(image)\n",
    "\n",
    "            # Calculating the loss function\n",
    "            loss = loss_function(reconstructed, image)\n",
    "\n",
    "            # Zero gradients, backward pass, and optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss for this batch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Log the loss to TensorBoard for each batch\n",
    "            global_step = epoch * len(loader) + batch_idx\n",
    "            writer.add_scalar(\"AE Loss/Batch\", loss.item(), global_step)\n",
    "\n",
    "        # Calculate and log average loss per epoch\n",
    "        average_loss = running_loss / len(loader)\n",
    "        writer.add_scalar(\"AE Loss/Epoch\", average_loss, epoch)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "l8Tdh6VDJbEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0544\n",
      "Epoch [2/5], Loss: 0.0337\n",
      "Epoch [3/5], Loss: 0.0282\n",
      "Epoch [4/5], Loss: 0.0263\n",
      "Epoch [5/5], Loss: 0.0252\n"
     ]
    }
   ],
   "source": [
    "tensor_transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root = \"./data\", train = True, download = True, transform = tensor_transform)\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = 30, shuffle = True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AE().to(device)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "train_AE(loader, device, model, loss_function, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=c:\\Users\\AjayK\\git\\VAE\\tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3158742353.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[58], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3\" pygame matplotlib\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3\" pygame matplotlib\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "env.obs_type = \"grayscale\"\n",
    "observation, info = env.reset()\n",
    "Image.fromarray(observation)\n",
    "device = torch.device(\"cpu\")\n",
    "height = observation.shape[0]\n",
    "width = observation.shape[1]\n",
    "channels = observation.shape[2]\n",
    "process = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "observation = process(observation)\n",
    "observation = observation.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 256, 11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, height=height, width=width):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent), latent.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "VAE_model = VAE().to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters())\n",
    "\n",
    "losses = []\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    observation = process(observation).unsqueeze(0).to(device)\n",
    "    reconstruction, latent = VAE_model(observation)\n",
    "    loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
