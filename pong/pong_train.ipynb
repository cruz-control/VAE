{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3[extra]\" pygame matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "objc[92137]: Class SDLApplication is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea5e8) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1088). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLAppDelegate is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea638) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d10d8). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLTranslatorResponder is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea6b0) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1150). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLMessageBoxPresenter is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea6d8) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1178). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDL_cocoametalview is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea728) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d11c8). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLOpenGLContext is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea778) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1218). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLWindow is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea908) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1268). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class Cocoa_WindowListener is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea930) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1290). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class SDLView is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0ea9a8) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1308). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class METAL_RenderData is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0eaa20) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d1358). One of the two will be used. Which one is undefined.\n",
      "objc[92137]: Class METAL_TextureData is implemented in both /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pygame/.dylibs/libSDL2-2.0.0.dylib (0x12b0eaa70) and /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ale_py/libSDL2-2.0.dylib (0x1364d13a8). One of the two will be used. Which one is undefined.\n",
      "2024-06-09 17:46:05.515 Python[92137:10179575] ApplePersistence=NO\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAFDElEQVR4Ae3cMW4UURCEYUAr2TkBh+AChGQ+CSEpl+EkZIS+AIdwQG4yQPKgSoDHm5JdXfwk0/LO6+2uj9FKtuH5s9/8ef/h9W9e4cuTEnj+mJDv3v79L83Hz18m5ffHWe+/ffrj6z9fvL66+es9Z254ceYwZ/MTADjf6NSEAJ+KL//w5alG1M/alc/mp5rT9b76Wbvy2ex6X55gV5KhfQAOhXGNBbArydA+AIfCuMYC2JVkaB+AQ2FcYwHsSjK0D8ChMK6xAHYlGdoH4FAY11gAu5IM7fNk34v+H77/rOaP+f1nfV+eYE2jsAa4EFVXAljTKKwf9XeyCvOLX4knOJ7o3IAAn8sv/vTl9u5r/JAMuJ8AT/B+diNOAjyCaX9IgPezG3ES4BFM+0MCvJ/diJMAj2DaHxLg/exGnAR4BNP+kADvZzfi5OXNq5cjBmXIvQR4gvdyG3MK4DFUe4MCvJfbmFMAj6HaGxTgvdzGnAJ4DNXeoADv5TbmFMBjqPYGBXgvtzGnAB5DtTfoA/CPX73jt+/2Egw/xRMcDnR2PIDPJhh+HuBwoLPjPfz7YH5oeDbI1PM8wakyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqG4BTZUxzAWwKMrUNwKkyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqG4BTZUxzAWwKMrUNwKkyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqm4f/Tjh1POb6mcD9t0+/gri+uvlVrxQ8wSspDb4H4MF4K6MDvJLS4HsAHoy3MjrAKykNvgfgwXgrowO8ktLgewAejLcyOsArKQ2+B+DBeCujA7yS0uB7AB6MtzI6wCspDb4H4MF4K6MDvJLS4Hv4efAAvH/9GbCuxBOsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWl9u7r4VrsdKRAE/wkUTpFeBS2GMtgI8kSq8Al8IeawF8JFF6BbgU9lgL4COJ0ivApbDHWgAfSZReAS6FPdYC+Eii9ApwKeyxFsBHEqVXgEthj7UAPpIovX4Hu1ci6IdaThkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "env.obs_type = \"grayscale\"\n",
    "observation, info = env.reset()\n",
    "Image.fromarray(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = observation.shape[0]\n",
    "width = observation.shape[1]\n",
    "channels = observation.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m observation \u001b[38;5;241m=\u001b[39m process(observation)\n\u001b[0;32m----> 2\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "observation = process(observation)\n",
    "observation = observation.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        return input\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 256, 11, 8)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, height=height, width=width):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent), latent.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Model Initialization\n",
    "VAE_model = VAE().to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 210, 160])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VAE_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(observation\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 2\u001b[0m x_hat, latent \u001b[38;5;241m=\u001b[39m \u001b[43mVAE_model\u001b[49m(observation)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_hat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(latent\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VAE_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(observation.shape)\n",
    "x_hat, latent = VAE_model(observation)\n",
    "print(x_hat.shape)\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    observation = process(observation).unsqueeze(0).to(device)\n",
    "    reconstruction, latent = VAE_model(observation)\n",
    "    loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "num_losses = []\n",
    "\n",
    "for x in losses:\n",
    "    num_losses.append(x.item())\n",
    "\n",
    "plt.plot(num_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.functional.to_pil_image(observation.view(3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_image = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_model = VAE().to(device)\n",
    "VAE_model.load_state_dict(torch.load(\"vae_model.pth\"))\n",
    "VAE_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentWrapper(gym.Env):\n",
    "    def __init__(self, other_env, size):\n",
    "        self.other_env = other_env\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(1, size), dtype=np.float32)\n",
    "        self.action_space = self.other_env.action_space\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observation, info = self.other_env.reset(seed=seed, options=options)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        before = transforms.functional.to_pil_image(observation.view(3, height, width))\n",
    "        after = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "\n",
    "        return latent, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.other_env.step(action)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return latent, reward, terminated, truncated, info\n",
    "        \n",
    "    def close(self):\n",
    "        self.other_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_env = LatentWrapper(env, 22528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", wrapper_env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"Pong\", env=wrapper_env)\n",
    "obs = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=40000, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = []\n",
    "win_count = 0\n",
    "\n",
    "obs, info = wrapper_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = wrapper_env.step(action)\n",
    "    if reward == 1:\n",
    "        win_count += 1\n",
    "    wins.append(win_count)\n",
    "    if terminated or truncated:\n",
    "        obs, info = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Wins')\n",
    "\n",
    "plt.plot(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Pong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
