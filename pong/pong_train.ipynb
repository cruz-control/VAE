{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.13.2)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3\" pygame matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "env.obs_type = \"grayscale\"\n",
    "observation, info = env.reset()\n",
    "Image.fromarray(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = observation.shape[0]\n",
    "width = observation.shape[1]\n",
    "channels = observation.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = process(observation)\n",
    "observation = observation.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        return input\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 256, 11, 8)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, height=height, width=width):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent), latent.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Model Initialization\n",
    "VAE_model = VAE().to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observation.shape)\n",
    "x_hat, latent = VAE_model(observation)\n",
    "print(x_hat.shape)\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    observation = process(observation).unsqueeze(0).to(device)\n",
    "    reconstruction, latent = VAE_model(observation)\n",
    "    loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "num_losses = []\n",
    "\n",
    "for x in losses:\n",
    "    num_losses.append(x.item())\n",
    "\n",
    "plt.plot(num_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.functional.to_pil_image(observation.view(3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_image = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_model = VAE().to(device)\n",
    "VAE_model.load_state_dict(torch.load(\"vae_model.pth\"))\n",
    "VAE_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentWrapper(gym.Env):\n",
    "    def __init__(self, other_env, size):\n",
    "        self.other_env = other_env\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(1, size), dtype=np.float32)\n",
    "        self.action_space = self.other_env.action_space\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observation, info = self.other_env.reset(seed=seed, options=options)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        before = transforms.functional.to_pil_image(observation.view(3, height, width))\n",
    "        after = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "\n",
    "        return latent, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.other_env.step(action)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return latent, reward, terminated, truncated, info\n",
    "        \n",
    "    def close(self):\n",
    "        self.other_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_env = LatentWrapper(env, 22528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", wrapper_env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"Pong\", env=wrapper_env)\n",
    "obs = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=40000, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = []\n",
    "win_count = 0\n",
    "\n",
    "obs, info = wrapper_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = wrapper_env.step(action)\n",
    "    if reward == 1:\n",
    "        win_count += 1\n",
    "    wins.append(win_count)\n",
    "    if terminated or truncated:\n",
    "        obs, info = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Wins')\n",
    "\n",
    "plt.plot(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Pong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
