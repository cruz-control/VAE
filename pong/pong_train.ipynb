{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install \"gymnasium[atari, accept-rom-license]\" opencv-python \"stable-baselines3[extra]\" pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAAFDElEQVR4Ae3cMW4UURCEYUAr2TkBh+AChGQ+CSEpl+EkZIS+AIdwQG4yQPKgSoDHm5JdXfwk0/LO6+2uj9FKtuH5s9/8ef/h9W9e4cuTEnj+mJDv3v79L83Hz18m5ffHWe+/ffrj6z9fvL66+es9Z254ceYwZ/MTADjf6NSEAJ+KL//w5alG1M/alc/mp5rT9b76Wbvy2ex6X55gV5KhfQAOhXGNBbArydA+AIfCuMYC2JVkaB+AQ2FcYwHsSjK0D8ChMK6xAHYlGdoH4FAY11gAu5IM7fNk34v+H77/rOaP+f1nfV+eYE2jsAa4EFVXAljTKKwf9XeyCvOLX4knOJ7o3IAAn8sv/vTl9u5r/JAMuJ8AT/B+diNOAjyCaX9IgPezG3ES4BFM+0MCvJ/diJMAj2DaHxLg/exGnAR4BNP+kADvZzfi5OXNq5cjBmXIvQR4gvdyG3MK4DFUe4MCvJfbmFMAj6HaGxTgvdzGnAJ4DNXeoADv5TbmFMBjqPYGBXgvtzGnAB5DtTfoA/CPX73jt+/2Egw/xRMcDnR2PIDPJhh+HuBwoLPjPfz7YH5oeDbI1PM8wakyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqG4BTZUxzAWwKMrUNwKkyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqG4BTZUxzAWwKMrUNwKkyprkANgWZ2gbgVBnTXACbgkxtA3CqjGkugE1BprYBOFXGNBfApiBT2wCcKmOaC2BTkKltAE6VMc0FsCnI1DYAp8qY5gLYFGRqm4f/Tjh1POb6mcD9t0+/gri+uvlVrxQ8wSspDb4H4MF4K6MDvJLS4HsAHoy3MjrAKykNvgfgwXgrowO8ktLgewAejLcyOsArKQ2+B+DBeCujA7yS0uB7AB6MtzI6wCspDb4H4MF4K6MDvJLS4Hv4efAAvH/9GbCuxBOsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWABei6koAaxqFNcCFqLoSwJpGYQ1wIaquBLCmUVgDXIiqKwGsaRTWl9u7r4VrsdKRAE/wkUTpFeBS2GMtgI8kSq8Al8IeawF8JFF6BbgU9lgL4COJ0ivApbDHWgAfSZReAS6FPdYC+Eii9ApwKeyxFsBHEqVXgEthj7UAPpIovX4Hu1ci6IdaThkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=160x210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\")\n",
    "env.obs_type = \"grayscale\"\n",
    "observation, info = env.reset()\n",
    "Image.fromarray(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = observation.shape[0]\n",
    "width = observation.shape[1]\n",
    "channels = observation.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = process(observation)\n",
    "observation = observation.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        return input\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), 256, 11, 8)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, height=height, width=width):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent), latent.to(\"cpu\").detach().numpy()\n",
    "\n",
    "# Model Initialization\n",
    "VAE_model = VAE().to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(VAE_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 210, 160])\n",
      "torch.Size([1, 3, 206, 158])\n",
      "(1, 22528)\n"
     ]
    }
   ],
   "source": [
    "print(observation.shape)\n",
    "x_hat, latent = VAE_model(observation)\n",
    "print(x_hat.shape)\n",
    "print(latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    observation = process(observation).unsqueeze(0).to(device)\n",
    "    reconstruction, latent = VAE_model(observation)\n",
    "    loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "num_losses = []\n",
    "\n",
    "for x in losses:\n",
    "    num_losses.append(x.item())\n",
    "\n",
    "plt.plot(num_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.functional.to_pil_image(observation.view(3, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_image = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VAE_model.state_dict(), \"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): Flatten()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): UnFlatten()\n",
       "    (1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE_model = VAE().to(device)\n",
    "VAE_model.load_state_dict(torch.load(\"vae_model.pth\"))\n",
    "VAE_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentWrapper(gym.Env):\n",
    "    def __init__(self, other_env, size):\n",
    "        self.other_env = other_env\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(1, size), dtype=np.float32)\n",
    "        self.action_space = self.other_env.action_space\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        observation, info = self.other_env.reset(seed=seed, options=options)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        before = transforms.functional.to_pil_image(observation.view(3, height, width))\n",
    "        after = transforms.functional.to_pil_image(reconstruction.view(3, 206, 158))\n",
    "\n",
    "        return latent, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.other_env.step(action)\n",
    "        \n",
    "        observation = process(observation).unsqueeze(0).to(device)\n",
    "        reconstruction, latent = VAE_model(observation)\n",
    "        \n",
    "        loss = loss_function(reconstruction, observation[:, :, :206, :158])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return latent, reward, terminated, truncated, info\n",
    "        \n",
    "    def close(self):\n",
    "        self.other_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_env = LatentWrapper(env, 22528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"Pong\", env=wrapper_env)\n",
    "obs = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = []\n",
    "win_count = 0\n",
    "\n",
    "obs, info = wrapper_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = wrapper_env.step(action)\n",
    "    if reward == 1:\n",
    "        win_count += 1\n",
    "    wins.append(win_count)\n",
    "    if terminated or truncated:\n",
    "        obs, info = wrapper_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Wins')\n",
    "\n",
    "plt.plot(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Pong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
